{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed issue with completion formatted as dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9129/9129 [00:00<00:00, 25155.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 9129/9129 [00:00<00:00, 340480.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified dataset saved to distillation-data/gold/llama3.3:70b-instruct-q4_K_M/earnest-grass-131-mod\n",
      "Original completion type: <class 'dict'>\n",
      "Modified completion type: <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import json\n",
    "\n",
    "def convert_completion_to_string(dataset_path, output_path):\n",
    "    \"\"\"\n",
    "    Load a dataset, convert 'completion' field from dict to string, and save the modified dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to the original dataset\n",
    "        output_path: Path where the modified dataset will be saved\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    \n",
    "    # Function to convert completion field from dict to string\n",
    "    def convert_completion(example):\n",
    "        if isinstance(example['completion'], dict):\n",
    "            # Convert the dictionary to a properly formatted JSON string\n",
    "            example['completion'] = json.dumps(example['completion'], indent=None, separators=(',', ':'))\n",
    "        return example\n",
    "    \n",
    "    # Apply the conversion function to each element\n",
    "    modified_dataset = dataset.map(convert_completion)\n",
    "    \n",
    "    # Save the modified dataset\n",
    "    modified_dataset.save_to_disk(output_path)\n",
    "    \n",
    "    print(f\"Modified dataset saved to {output_path}\")\n",
    "    print(f\"Original completion type: {type(dataset[0]['completion'])}\")\n",
    "    print(f\"Modified completion type: {type(modified_dataset[0]['completion'])}\")\n",
    "    \n",
    "    return modified_dataset\n",
    "\n",
    "# Execute the function\n",
    "original_path = \"distillation-data/gold/llama3.3:70b-instruct-q4_K_M/earnest-grass-131\"\n",
    "output_path = \"distillation-data/gold/llama3.3:70b-instruct-q4_K_M/earnest-grass-131-mod\"\n",
    "modified_dataset = convert_completion_to_string(original_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test ECTSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.data.data_manager import SummaryManager\n",
    "\n",
    "prompts, true_labels, pred_labels = SummaryManager.load_data(\"summary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Majority "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price_or_not': 1,\n",
       " 'price_up': 0,\n",
       " 'price_const_stable': 0,\n",
       " 'price_down': 1,\n",
       " 'past_price_info': 1,\n",
       " 'future_price_info': 1,\n",
       " 'past_gen_info': 0,\n",
       " 'future_gen_info': 0,\n",
       " 'asset_comparison': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.query_utils import find_majority_dict\n",
    "\n",
    "example_1 = {\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 1,\n",
    "    \"price_down\": 0,\n",
    "    \"past_price_info\": 0,\n",
    "    \"future_price_info\": 1,\n",
    "    \"past_gen_info\": 0,\n",
    "    \"future_gen_info\": 0,\n",
    "    \"asset_comparison\": 0\n",
    "}\n",
    "\n",
    "example_2 = {\n",
    "    \"price_or_not\": 0,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 0,\n",
    "    \"price_down\": 1,\n",
    "    \"past_price_info\": 1,\n",
    "    \"future_price_info\": 0,\n",
    "    \"past_gen_info\": 1,\n",
    "    \"future_gen_info\": 0,\n",
    "    \"asset_comparison\": 1\n",
    "}\n",
    "\n",
    "example_3 = {\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 0,\n",
    "    \"price_down\": 1,\n",
    "    \"past_price_info\": 1,\n",
    "    \"future_price_info\": 1,\n",
    "    \"past_gen_info\": 0,\n",
    "    \"future_gen_info\": 0,\n",
    "    \"asset_comparison\": 1\n",
    "}\n",
    "\n",
    "examples = [example_1, example_2, example_3]\n",
    "\n",
    "find_majority_dict(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test performance evaluation for gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Overall Metrics ===\n",
      "Accuracy: 0.7778\n",
      "Balanced Accuracy: 0.7500\n",
      "F1 Micro: 0.7778\n",
      "Total samples: 27\n",
      "\n",
      "=== Per-Category Metrics ===\n",
      "          Category  Accuracy  Balanced Acc  F1 Micro  Samples\n",
      "      price_or_not       1.0           1.0       1.0        3\n",
      "          price_up       1.0           1.0       1.0        3\n",
      "price_const_stable       0.0           0.0       0.0        3\n",
      "        price_down       1.0           1.0       1.0        3\n",
      "   past_price_info       0.0           0.0       0.0        3\n",
      " future_price_info       1.0           1.0       1.0        3\n",
      "     past_gen_info       1.0           1.0       1.0        3\n",
      "   future_gen_info       1.0           1.0       1.0        3\n",
      "  asset_comparison       1.0           1.0       1.0        3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "import wandb\n",
    "\n",
    "def evaluate_multilabel_performance(true_labels: List[Dict[str, int]], pred_labels: List[Dict[str, int]], wandb_run = None) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate multi-label classification performance.\n",
    "    \n",
    "    Args:\n",
    "        true_labels: List of dictionaries containing true labels\n",
    "        pred_labels: List of dictionaries containing predicted labels\n",
    "        wandb_run: Optional Weights & Biases run object for logging\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing performance metrics\n",
    "    \"\"\"\n",
    "    # Determine categories if not provided\n",
    "    categories = list(true_labels[0].keys())\n",
    "    \n",
    "    # Initialize metric trackers\n",
    "    metrics_per_category = {}\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    \n",
    "    # Evaluate each category separately\n",
    "    for category in categories:\n",
    "        category_true = []\n",
    "        category_pred = []\n",
    "        \n",
    "        for true_dict, pred_dict in zip(true_labels, pred_labels):\n",
    "            # Skip if category missing in either dictionary\n",
    "            if category not in true_dict or category not in pred_dict:\n",
    "                continue\n",
    "                \n",
    "            true_val = true_dict[category]\n",
    "            \n",
    "            # Handle missing category in pred_dict by treating as incorrect\n",
    "            if category not in pred_dict:\n",
    "                # Force an incorrect prediction\n",
    "                pred_val = 1 - true_val if true_val in (0, 1) else 0\n",
    "            else:\n",
    "                pred_val = pred_dict[category]\n",
    "                \n",
    "                # IMPORTANT CHANGE: Penalize missing predictions (-1) \n",
    "                # by replacing with a value guaranteed to be incorrect\n",
    "                if pred_val == -1:\n",
    "                    # Convert to opposite of true value to ensure it's wrong\n",
    "                    pred_val = 1 - true_val if true_val in (0, 1) else 0\n",
    "\n",
    "            category_true.append(true_val)\n",
    "            category_pred.append(pred_val)\n",
    "            \n",
    "            # Add to overall evaluation\n",
    "            all_true.append(true_val)\n",
    "            all_pred.append(pred_val)\n",
    "        \n",
    "        # Calculate metrics for this category\n",
    "        if category_true and category_pred:\n",
    "            try:\n",
    "                accuracy = accuracy_score(category_true, category_pred)\n",
    "                # Only calculate balanced_accuracy if there are at least two classes\n",
    "                unique_classes = len(set(category_true))\n",
    "                if unique_classes > 1:\n",
    "                    balanced_acc = balanced_accuracy_score(category_true, category_pred)\n",
    "                else:\n",
    "                    balanced_acc = accuracy  # Fall back to regular accuracy\n",
    "                    \n",
    "                f1_micro = f1_score(category_true, category_pred, average='micro')\n",
    "                \n",
    "                metrics_per_category[category] = {\n",
    "                    'accuracy': accuracy,\n",
    "                    'balanced_accuracy': balanced_acc,\n",
    "                    'f1_micro': f1_micro,\n",
    "                    'samples': len(category_true)\n",
    "                }\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics for category {category}: {e}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_metrics = {}\n",
    "    if all_true and all_pred:\n",
    "        overall_metrics['accuracy'] = accuracy_score(all_true, all_pred)\n",
    "        overall_metrics['balanced_accuracy'] = balanced_accuracy_score(all_true, all_pred)\n",
    "        overall_metrics['f1_micro'] = f1_score(all_true, all_pred, average='micro')\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Overall Metrics ===\")\n",
    "    print(f\"Accuracy: {overall_metrics.get('accuracy', 0):.4f}\")\n",
    "    print(f\"Balanced Accuracy: {overall_metrics.get('balanced_accuracy', 0):.4f}\")\n",
    "    print(f\"F1 Micro: {overall_metrics.get('f1_micro', 0):.4f}\")\n",
    "    \n",
    "    metrics_df = pd.DataFrame([\n",
    "        {\n",
    "            'Category': cat,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Balanced Acc': metrics['balanced_accuracy'],\n",
    "            'F1 Micro': metrics['f1_micro'],\n",
    "            'Samples': metrics['samples']\n",
    "        }\n",
    "        for cat, metrics in metrics_per_category.items()\n",
    "    ])\n",
    "    \n",
    "    if wandb_run:\n",
    "        wandb_run.log({\"accuracy\": overall_metrics.get('accuracy', 0)})\n",
    "        wandb_run.log({\"balanced_accuracy\": overall_metrics.get('balanced_accuracy', 0)})\n",
    "        wandb_run.log({\"f1_micro\": overall_metrics.get('f1_micro', 0)})\n",
    "        wandb_run.log({\"metrics_df\": wandb.Table(dataframe=metrics_df)})\n",
    "\n",
    "true_label = {\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 1,\n",
    "    \"price_down\": 0,\n",
    "    \"past_price_info\": 0,\n",
    "    \"future_price_info\": 1,\n",
    "    \"past_gen_info\": 0,\n",
    "    \"future_gen_info\": 0,\n",
    "    \"asset_comparison\": 0\n",
    "}\n",
    "\n",
    "pred_label = {\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 0,  # Incorrect prediction\n",
    "    \"price_down\": 0,\n",
    "    \"past_price_info\": -1,    # Missing prediction\n",
    "    \"future_price_info\": 1,\n",
    "    \"past_gen_info\": 0,\n",
    "    \"future_gen_info\": 0,\n",
    "    \"asset_comparison\": 0\n",
    "}\n",
    "\n",
    "# For demonstration, create a list with multiple examples\n",
    "true_labels = [true_label, true_label, true_label]\n",
    "pred_labels = [pred_label, pred_label, pred_label]\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_multilabel_performance(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean LLM Output for Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['price_or_not', 'price_up', 'price_const_stable', 'price_down', 'past_price_info', 'future_price_info', 'past_gen_info', 'future_gen_info', 'asset_comparison'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {\n",
    "\"price_or_not\": 1,\n",
    "\"price_up\": 0,\n",
    "\"price_const_stable\": 1,\n",
    "\"price_down\": 0,\n",
    "\"past_price_info\": 0,\n",
    "\"future_price_info\": 1,\n",
    "\"past_gen_info\": 0,\n",
    "\"future_gen_info\": 0,\n",
    "\"asset_comparison\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def process_output(input_data):\n",
    "    \"\"\"\n",
    "    Process various input formats containing financial sentiment data and return a standardized dictionary.\n",
    "    \n",
    "    Args:\n",
    "        input_data: Dictionary, string, or other format containing sentiment analysis results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with all expected keys and validated values (0, 1, or -1 for errors)\n",
    "    \"\"\"\n",
    "    # Define the expected keys\n",
    "    expected_keys = [\n",
    "        \"price_or_not\", \n",
    "        \"price_up\", \n",
    "        \"price_const_stable\", \n",
    "        \"price_down\", \n",
    "        \"past_price_info\", \n",
    "        \"future_price_info\", \n",
    "        \"past_gen_info\", \n",
    "        \"future_gen_info\", \n",
    "        \"asset_comparison\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize result dictionary with all keys set to -1\n",
    "    result = {key: -1 for key in expected_keys}\n",
    "    \n",
    "    # Parse input to dictionary if it's not already\n",
    "    parsed_data = {}\n",
    "    \n",
    "    if isinstance(input_data, dict):\n",
    "        parsed_data = input_data\n",
    "    elif isinstance(input_data, str):\n",
    "        # Remove markdown code blocks if present\n",
    "        clean_input = re.sub(r'```(?:json|python)?\\s*|\\s*```', '', input_data)\n",
    "        \n",
    "        # Try to find and extract JSON-like structure from text\n",
    "        json_pattern = r'(?:\\{|\\[).*?(?:\\}|\\])'\n",
    "        json_matches = re.findall(json_pattern, clean_input, re.DOTALL)\n",
    "        \n",
    "        if json_matches:\n",
    "            # Try each potential JSON match\n",
    "            for json_str in json_matches:\n",
    "                try:\n",
    "                    candidate = json.loads(json_str)\n",
    "                    if isinstance(candidate, dict) and any(key in candidate for key in expected_keys):\n",
    "                        parsed_data = candidate\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        \n",
    "        # If no valid JSON was found, try direct parsing\n",
    "        if not parsed_data:\n",
    "            try:\n",
    "                parsed_data = json.loads(clean_input)\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to parse Python dict syntax\n",
    "                try:\n",
    "                    # Replace single quotes with double quotes for JSON parsing\n",
    "                    clean_input = clean_input.replace(\"'\", '\"')\n",
    "                    parsed_data = json.loads(clean_input)\n",
    "                except json.JSONDecodeError:\n",
    "                    # Try to extract key-value pairs using regex\n",
    "                    pairs = re.findall(r'\"?(\\w+)\"?\\s*:\\s*(-?\\d+)', clean_input)\n",
    "                    parsed_data = {key: int(value) for key, value in pairs}\n",
    "    \n",
    "    # Update result with valid values from parsed data\n",
    "    for key in expected_keys:\n",
    "        if key in parsed_data:\n",
    "            # Ensure value is 0 or 1, otherwise set to -1\n",
    "            if parsed_data[key] in [0, 1]:\n",
    "                result[key] = parsed_data[key]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'price_or_not': 1, 'price_up': 0, 'price_const_stable': 1, 'price_down': 0, 'past_price_info': 0, 'future_price_info': 1, 'past_gen_info': -1, 'future_gen_info': -1, 'asset_comparison': -1}\n",
      "{'price_or_not': 1, 'price_up': 0, 'price_const_stable': 1, 'price_down': -1, 'past_price_info': -1, 'future_price_info': -1, 'past_gen_info': -1, 'future_gen_info': -1, 'asset_comparison': -1}\n",
      "{'price_or_not': 1, 'price_up': 0, 'price_const_stable': 1, 'price_down': 0, 'past_price_info': -1, 'future_price_info': -1, 'past_gen_info': -1, 'future_gen_info': -1, 'asset_comparison': -1}\n",
      "{'price_or_not': -1, 'price_up': -1, 'price_const_stable': -1, 'price_down': -1, 'past_price_info': -1, 'future_price_info': -1, 'past_gen_info': -1, 'future_gen_info': -1, 'asset_comparison': -1}\n",
      "{'price_or_not': 1, 'price_up': 0, 'price_const_stable': 1, 'price_down': 0, 'past_price_info': 0, 'future_price_info': 1, 'past_gen_info': -1, 'future_gen_info': -1, 'asset_comparison': -1}\n",
      "{'price_or_not': 1, 'price_up': 0, 'price_const_stable': 1, 'price_down': 0, 'past_price_info': 0, 'future_price_info': 1, 'past_gen_info': -1, 'future_gen_info': -1, 'asset_comparison': -1}\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Dictionary input\n",
    "output1 = {\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 1,\n",
    "    \"price_down\": 0,\n",
    "    \"past_price_info\": 0,\n",
    "    \"future_price_info\": 1\n",
    "}\n",
    "result1 = process_output(output1)\n",
    "print(result1)  # Missing keys will have value -1\n",
    "\n",
    "# Example 2: JSON string input\n",
    "output2 = '{\"price_or_not\": 1, \"price_up\": 0, \"price_const_stable\": 1}'\n",
    "result2 = process_output(output2)\n",
    "print(result2)\n",
    "\n",
    "# Example 3: Markdown code block\n",
    "output3 = '''```json\n",
    "{\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 1,\n",
    "    \"price_down\": 0\n",
    "}\n",
    "```'''\n",
    "result3 = process_output(output3)\n",
    "print(result3)\n",
    "\n",
    "# Example 4: Invalid values\n",
    "output4 = {\"price_or_not\": 2, \"price_up\": \"yes\"}\n",
    "result4 = process_output(output4)\n",
    "print(result4)  # All values will be -1 as they're invalid\n",
    "\n",
    "# Example with JSON embedded in text\n",
    "mixed_text = \"\"\"\n",
    "After analyzing the financial news, here's my assessment:\n",
    "\n",
    "{\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 1,\n",
    "    \"price_down\": 0,\n",
    "    \"past_price_info\": 0,\n",
    "    \"future_price_info\": 1\n",
    "}\n",
    "\n",
    "I hope this information is helpful for your investment decisions.\n",
    "\"\"\"\n",
    "\n",
    "result = process_output(mixed_text)\n",
    "print(result)\n",
    "\n",
    "# Incomplete JSON example\n",
    "incomplete_json = '''\n",
    "{\n",
    "    \"price_or_not\": 1,\n",
    "    \"price_up\": 0,\n",
    "    \"price_const_stable\": 1,\n",
    "    \"price_down\": 0,\n",
    "    \"past_price_info\": 0,\n",
    "    \"future_price_info\": 1\n",
    "'''\n",
    "result = process_output(incomplete_json)\n",
    "print(result)  # Should handle incomplete JSON gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test gold dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.data.data_manager import GoldDataManager\n",
    "\n",
    "prompts, true_labels, pred_labels = GoldDataManager.load_data(\"gold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model outputs on train / valid / test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from src.models.hf_utils import HF_Manager\n",
    "from src.data.data_transforms import DataTransforms\n",
    "\n",
    "model_output_dir = \"models/sentiment:50agree/llama3.2:1b/deft-pyramid-98\"\n",
    "\n",
    "dataset = load_from_disk(f\"distillation-data/sentiment:allagree/llama3.1:405b/silver-grass-20-mod\") # load the distillation dataset \n",
    "dataset = DataTransforms.split_data(dataset) # split the train dataset into train and test/valid sets\n",
    "train_dataset = dataset[\"train\"]\n",
    "valid_dataset = dataset[\"test\"]\n",
    "test_dataset = load_from_disk(f\"data/sentiment:50agree/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test inference with model models/sentiment:50agree/llama3.2:1b/deft-pyramid-98 on dataset (3100, 3). Limit: 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 1/2 [00:53<00:53, 53.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: The Board of Directors has proposed the Extraordinary General Meeting to authorise the Board to decide on the issuance of a maximum of 30mn new shares in one or more share issues .\n",
      "Final Label: \n",
      "Completion by student: Neutral\n",
      "Completion by teacher: neutral\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:46<00:00, 53.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: Finnish forest machinery manufacturer Ponsse 's net sales grew to EUR 51.3 mn in the first quarter of 2010 from EUR 37.5 mn in the corresponding period in 2009 .\n",
      "Final Label: \n",
      "Completion by student: positive\n",
      "Completion by teacher: positive\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "HF_Manager.predict(model_path=model_output_dir, dataset=train_dataset, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test inference with model models/sentiment:50agree/llama3.2:1b/deft-pyramid-98 on dataset (776, 3). Limit: 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 1/2 [04:34<04:34, 274.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: The mill has long traditions and holds an established position in the markets .\n",
      "Final Label: \n",
      "Completion by student: neutral\n",
      "Completion by teacher: positive\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 2/2 [06:49<00:00, 204.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: In the Baltic states the company reports net sales of EUR 11.9 mn , down from EUR 14.2 mn , and an operative EBIT of EUR -2.2 mn , down from EUR -1.7 mn .\n",
      "Final Label: \n",
      "Completion by student: neutral\n",
      "Completion by teacher: negative\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "HF_Manager.predict(model_path=model_output_dir, dataset=valid_dataset, limit=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test inference with model models/sentiment:50agree/llama3.2:1b/deft-pyramid-98 on dataset (970, 2). Limit: 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 1/2 [00:45<00:45, 45.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion before processing: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: Profit before taxes was EUR 5.4 mn , up from EUR 3.6 mn a year earlier .\n",
      "Final Label:  negative\n",
      "Example 0:\n",
      "Prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: Profit before taxes was EUR 5.4 mn , up from EUR 3.6 mn a year earlier .\n",
      "Final Label: \n",
      "Completion by student: negative\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:31<00:00, 45.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion before processing: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: Kaupthing Bank will publish its annual results for 2007 before markets open on Thursday 31 January .\n",
      "Final Label:  neutral\n",
      "Example 1:\n",
      "Prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: Kaupthing Bank will publish its annual results for 2007 before markets open on Thursday 31 January .\n",
      "Final Label: \n",
      "Completion by student: neutral\n",
      "-----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "HF_Manager.predict(model_path=model_output_dir, dataset=test_dataset, limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify prompt to end with 'Final Label:'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3876/3876 [00:00<00:00, 34116.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3876/3876 [00:00<00:00, 1041055.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(f\"models/sentiment:50agree/llama3.1:405b/inference_outputs/silver-grass-20\")\n",
    "\n",
    "# Define a function to modify the 'prompt' column\n",
    "def modify_prompt(example):\n",
    "    last_index = example['prompt'].rfind('Label: ')\n",
    "    if last_index != -1:\n",
    "        example['prompt'] = example['prompt'][:last_index] + 'Final Label: ' + example['prompt'][last_index + len('Label: '):]\n",
    "    return example\n",
    "\n",
    "# Apply the transformation using map\n",
    "dataset = dataset.map(modify_prompt)\n",
    "\n",
    "# Save the modified dataset\n",
    "dataset.save_to_disk(\"models/sentiment:50agree/llama3.1:405b/inference_outputs/silver-grass-20-mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a highly qualified expert trained to annotate machine learning training data.\\n\\nYour task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\\npositive, negative, or neutral.\\n\\nBase your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \\n\\nDo not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\\n\\nExamples:\\nText: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\\nLabel: positive\\nText: The company generated net sales of 11.3 million euro this year.\\nLabel: neutral\\nText: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\\t\\nLabel: negative\\n\\nYour TEXT to analyse:\\nTEXT: Sanoma Magazines International will invite other shareholders holding approximately 15 % of the shares to sell their shares .\\nFinal Label: '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(f\"models/sentiment:50agree/llama3.1:405b/inference_outputs/silver-grass-20-mod\")\n",
    "dataset[0]['prompt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push model to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 4.94G/4.94G [04:02<00:00, 20.4MB/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hendrik-spl/test_honest-serenity-73/commit/9af4b95dac2fafc3bc99f5991c7756c82027998f', commit_message='Upload LlamaForCausalLM', commit_description='', oid='9af4b95dac2fafc3bc99f5991c7756c82027998f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hendrik-spl/test_honest-serenity-73', endpoint='https://huggingface.co', repo_type='model', repo_id='hendrik-spl/test_honest-serenity-73'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push to Hugging Face Hub\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"models/sentiment:50agree/llama3.2:1b/checkpoints/honest-serenity-73\"\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "hf_model.push_to_hub(\"hendrik-spl/test_honest-serenity-73\")\n",
    "hf_tokenizer.push_to_hub(\"hendrik-spl/test_honest-serenity-73\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing token length of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 709.9885208552402\n",
      "Max length: 735\n",
      "% truncated at 256: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# INCLUDING PROMPT\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.prompts.gold import get_gold_classification_prompt\n",
    "from src.data.data_manager import GoldDataManager\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = GoldDataManager.load_original_data()\n",
    "\n",
    "lengths = [len(tokenizer.encode(get_gold_classification_prompt(example['News']))) for example in dataset['train']]\n",
    "print(f\"Average length: {sum(lengths)/len(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"% truncated at 256: {sum(l > 768 for l in lengths)/len(lengths):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 14.126883981773572\n",
      "Max length: 39\n",
      "% truncated at 256: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT PROMPT\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.prompts.gold import get_gold_classification_prompt\n",
    "from src.data.data_manager import GoldDataManager\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = GoldDataManager.load_original_data()\n",
    "\n",
    "lengths = [len(tokenizer.encode(example['News'])) for example in dataset['train']]\n",
    "print(f\"Average length: {sum(lengths)/len(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"% truncated at 256: {sum(l > 256 for l in lengths)/len(lengths):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length: 30.292818819645067\n",
      "Max length: 135\n",
      "% truncated at 256: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT PROMPT\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "dataset = load_dataset(path=\"takala/financial_phrasebank\", name=\"sentences_50agree\", trust_remote_code=True)\n",
    "\n",
    "lengths = [len(tokenizer.encode(example['sentence'])) for example in dataset['train']]\n",
    "print(f\"Average length: {sum(lengths)/len(lengths)}\")\n",
    "print(f\"Max length: {max(lengths)}\")\n",
    "print(f\"% truncated at 256: {sum(l > 256 for l in lengths)/len(lengths):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY OUTPUT\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "example = '''{\n",
    "\"price_or_not\": 0,\n",
    "\"price_up\": 0,\n",
    "\"price_const_stable\": 1,\n",
    "\"price_down\": 1,\n",
    "\"past_price_info\": 0,\n",
    "\"future_price_info\": 0,\n",
    "\"past_gen_info\": 0,\n",
    "\"future_gen_info\": 0,'''\n",
    "\n",
    "lengths = len(tokenizer.encode(example))\n",
    "print(f\"Length: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the outputs of different models across HF and ollama on sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer meta-llama/Llama-3.2-1B-Instruct does not have a pad token. Setting pad token to eos token.\n",
      "Running model meta-llama/Llama-3.2-1B-Instruct with prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: After a long up and down, the stock market is finally on the rise, says the analyst.\n",
      "Label: \n",
      "Implemented stopping criteria: [<src.models.hf_stopping.KeywordStoppingCriteria object at 0x31fe942d0>]\n",
      "Response 1:\n",
      "Reponse:  neutral\n",
      "TEXT:\n",
      "Cleaned response: neutral\n",
      "------------\n",
      "Implemented stopping criteria: [<src.models.hf_stopping.KeywordStoppingCriteria object at 0x31eafce90>]\n",
      "Response 2:\n",
      "Reponse:  neutral\n",
      "TEXT:\n",
      "Cleaned response: neutral\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untrained llama straight from hf but with KeyWordStoppingCriteria implemented in query_hf_model\n",
    "\n",
    "from src.models.model_utils import query_with_sc\n",
    "from src.prompts.sentiment import get_sentiment_prompt\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Tokenizer {tokenizer.name_or_path} does not have a pad token. Setting pad token to eos token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Make sure we're setting a single integer ID\n",
    "    if isinstance(model.config.eos_token_id, list):\n",
    "        # If eos_token_id is a list, use the first element\n",
    "        model.config.pad_token_id = model.config.eos_token_id[0]\n",
    "    else:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "model_config = (model, tokenizer)\n",
    "\n",
    "prompt = get_sentiment_prompt(\"After a long up and down, the stock market is finally on the rise, says the analyst.\")\n",
    "\n",
    "print(f\"Running model {model_path} with prompt: {prompt}\")\n",
    "response = query_with_sc(\n",
    "    model=model_config,\n",
    "    prompt=prompt,\n",
    "    shots=2,\n",
    "    use_ollama=False,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Reponse: neutral\n",
      "Cleaned response: neutral\n",
      "------------\n",
      "Response 2:\n",
      "Reponse: neutral\n",
      "Cleaned response: neutral\n",
      "------------\n",
      "Response 3:\n",
      "Reponse: negative\n",
      "Cleaned response: negative\n",
      "------------\n",
      "Response 4:\n",
      "Reponse: negative\n",
      "Cleaned response: negative\n",
      "------------\n",
      "Response 5:\n",
      "Reponse: negative\n",
      "Cleaned response: negative\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ollama llama3.2:1b\n",
    "\n",
    "from src.models.model_utils import query_with_sc\n",
    "from src.prompts.sentiment import get_sentiment_prompt\n",
    "\n",
    "model_config = \"llama3.2:1b\"\n",
    "\n",
    "prompt = get_sentiment_prompt(\"After a long up and down, the stock market is finally on the rise, says the analyst.\")\n",
    "\n",
    "response = query_with_sc(\n",
    "    model=model_config,\n",
    "    prompt=prompt,\n",
    "    shots=5,\n",
    "    use_ollama=True,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer meta-llama/Llama-3.2-1B-Instruct does not have a pad token. Setting pad token to eos token.\n",
      "Running model meta-llama/Llama-3.2-1B-Instruct with prompt: You are a highly qualified expert trained to annotate machine learning training data.\n",
      "\n",
      "Your task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\n",
      "positive, negative, or neutral.\n",
      "\n",
      "Base your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \n",
      "\n",
      "Do not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\n",
      "\n",
      "Examples:\n",
      "Text: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\n",
      "Label: positive\n",
      "Text: The company generated net sales of 11.3 million euro this year.\n",
      "Label: neutral\n",
      "Text: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\t\n",
      "Label: negative\n",
      "\n",
      "Your TEXT to analyse:\n",
      "TEXT: After a long up and down, the stock market is finally on the rise, says the analyst.\n",
      "Label: \n",
      "Response 1:\n",
      "Reponse:  neutral\n",
      "TEXT: The company has been struggling to find a new investor, despite having a strong financial\n",
      "Cleaned response: neutral\n",
      "------------\n",
      "Response 2:\n",
      "Reponse:  positive\n",
      "TEXT: The company's financials are looking good, with a 20% increase in\n",
      "Cleaned response: positive\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Untrained llama straight from hf\n",
    "\n",
    "from src.models.model_utils import query_with_sc\n",
    "from src.prompts.sentiment import get_sentiment_prompt\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(f\"Tokenizer {tokenizer.name_or_path} does not have a pad token. Setting pad token to eos token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Make sure we're setting a single integer ID\n",
    "    if isinstance(model.config.eos_token_id, list):\n",
    "        # If eos_token_id is a list, use the first element\n",
    "        model.config.pad_token_id = model.config.eos_token_id[0]\n",
    "    else:\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "model_config = (model, tokenizer)\n",
    "\n",
    "prompt = get_sentiment_prompt(\"After a long up and down, the stock market is finally on the rise, says the analyst.\")\n",
    "\n",
    "print(f\"Running model {model_path} with prompt: {prompt}\")\n",
    "response = query_with_sc(\n",
    "    model=model_config,\n",
    "    prompt=prompt,\n",
    "    shots=2,\n",
    "    use_ollama=False,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Reponse:  positive\n",
      "TEXT: The company's financials are looking better, with a 10% increase in\n",
      "Cleaned response: positive\n",
      "------------\n",
      "Response 2:\n",
      "Reponse:  positive\n",
      "TEXT: The company has been struggling to meet its quarterly targets, and its stock price has\n",
      "Cleaned response: positive\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Untrained hf base model\n",
    "\n",
    "from src.models.model_utils import query_with_sc\n",
    "from src.prompts.sentiment import get_sentiment_prompt\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"models/sentiment:50agree/llama3.2:1b/checkpoints/dazzling-forest-66\" # base model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_config = (model, tokenizer)\n",
    "\n",
    "prompt = get_sentiment_prompt(\"After a long up and down, the stock market is finally on the rise, says the analyst.\")\n",
    "\n",
    "response = query_with_sc(\n",
    "    model=model_config,\n",
    "    prompt=prompt,\n",
    "    shots=2,\n",
    "    use_ollama=False,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:  positive\n",
      "TEXT: The company's financials are looking better than expected, but the market is still\n",
      "Reponse:  positive\n",
      "TEXT: The company's financials are looking better than expected, but the market is still\n",
      "Cleaned response: positive\n",
      "------------\n",
      "Response 2:  neutral\n",
      "TEXT: The company has been struggling to turn a profit for several years, and the recent\n",
      "Reponse:  neutral\n",
      "TEXT: The company has been struggling to turn a profit for several years, and the recent\n",
      "Cleaned response: neutral\n",
      "------------\n",
      "Response 3:  positive\n",
      "TEXT: The company has been struggling to stay afloat, despite efforts to improve its financial\n",
      "Reponse:  positive\n",
      "TEXT: The company has been struggling to stay afloat, despite efforts to improve its financial\n",
      "Cleaned response: positive\n",
      "------------\n",
      "Response 4:  neutral\n",
      "TEXT: The company has been struggling to make ends meet, despite its efforts to increase production\n",
      "Reponse:  neutral\n",
      "TEXT: The company has been struggling to make ends meet, despite its efforts to increase production\n",
      "Cleaned response: neutral\n",
      "------------\n",
      "Response 5:  neutral\n",
      "TEXT: The company has been struggling to maintain its market share in the highly competitive tech industry\n",
      "Reponse:  neutral\n",
      "TEXT: The company has been struggling to maintain its market share in the highly competitive tech industry\n",
      "Cleaned response: neutral\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Finetuned hf model\n",
    "\n",
    "from src.models.model_utils import query_with_sc\n",
    "from src.prompts.sentiment import get_sentiment_prompt\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"models/sentiment:50agree/llama3.2:1b/checkpoints/smart-cosmos-63\" # finetuned model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model_config = (model, tokenizer)\n",
    "\n",
    "prompt = get_sentiment_prompt(\"After a long up and down, the stock market is finally on the rise, says the analyst.\")\n",
    "\n",
    "response = query_with_sc(\n",
    "    model=model_config,\n",
    "    prompt=prompt,\n",
    "    shots=2,\n",
    "    use_ollama=False,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing proper HF inference calls and understanding different approaches to eos and pad token config stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I've\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def query_hf_model(model_path, prompt):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Store the input length to know where the generated text starts\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "    # Generate a response\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs,\n",
    "                                 pad_token_id=tokenizer.eos_token_id,\n",
    "                                 max_new_tokens=2\n",
    "                                 )\n",
    "\n",
    "    # Decode ONLY the generated tokens (exclude the input prompt tokens)\n",
    "    response = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n",
    "    \n",
    "    return response\n",
    "\n",
    "query_hf_model(\"models/sentiment:50agree/llama3.2:1b/checkpoints/smart-cosmos-63\", \"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current generation config: {'bos_token_id': 128000, 'do_sample': True, 'eos_token_id': [128001, 128008, 128009], 'temperature': 0.6, 'top_p': 0.9, 'transformers_version': '4.49.0'}\n",
      "Found list in eos_token_id: [128001, 128008, 128009]\n",
      "Fixed generation config saved to: models/sentiment:50agree/llama3.2:1b/checkpoints/sparkling-waterfall-43/generation_config.json.fixed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if a generation_config.json exists and examine it\n",
    "gen_config_path = os.path.join(model_path, \"generation_config.json\")\n",
    "if os.path.exists(gen_config_path):\n",
    "    with open(gen_config_path, 'r') as f:\n",
    "        gen_config = json.load(f)\n",
    "        print(\"Current generation config:\", gen_config)\n",
    "        \n",
    "        # Look for list parameters that should be integers\n",
    "        for key, value in gen_config.items():\n",
    "            if isinstance(value, list) and key in [\"pad_token_id\", \"eos_token_id\", \"bos_token_id\"]:\n",
    "                print(f\"Found list in {key}: {value}\")\n",
    "                # Fix by converting to integer if needed\n",
    "                if value:\n",
    "                    gen_config[key] = value[0]\n",
    "                else:\n",
    "                    gen_config[key] = None\n",
    "        \n",
    "        # Save the fixed config\n",
    "        with open(gen_config_path + \".fixed\", 'w') as f:\n",
    "            json.dump(gen_config, f, indent=2)\n",
    "            \n",
    "        print(\"Fixed generation config saved to:\", gen_config_path + \".fixed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SentimentDataManager class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory models/sentiment:50agree/smollm2:135m/inference_outputs/test123.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 1/1 [00:00<00:00, 140.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from src.data.data_manager import SentimentDataManager\n",
    "\n",
    "SentimentDataManager.save_model_outputs(\n",
    "    [\"hello\"],\n",
    "    [\"positive\"],\n",
    "    [\"neutral\"],\n",
    "    \"sentiment:50agree\",\n",
    "    \"smollm2:135m\",\n",
    "    \"test123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure\n",
      "Dataset({\n",
      "    features: ['prompt', 'true_label', 'completion'],\n",
      "    num_rows: 10\n",
      "})\n",
      "First row\n",
      "{'prompt': 'You are a highly qualified expert trained to annotate machine learning training data.\\n\\nYour task is to analyze the sentiment in the TEXT below from an investor perspective and label it with only one the three labels:\\npositive, negative, or neutral.\\n\\nBase your label decision only on the TEXT and do not speculate e.g. based on prior knowledge about a company. \\n\\nDo not provide any explanations and only respond with one of the labels as one word: negative, positive, or neutral\\n\\nExamples:\\nText: Operating profit increased, from EUR 7m to 9m compared to the previous reporting period.\\nLabel: positive\\nText: The company generated net sales of 11.3 million euro this year.\\nLabel: neutral\\nText: Profit before taxes decreased to EUR 14m, compared to EUR 19m in the previous period.\\t\\nLabel: negative\\n\\nYour TEXT to analyse:\\nTEXT: Sanoma Magazines International will invite other shareholders holding approximately 15 % of the shares to sell their shares .\\nLabel: ', 'true_label': 1, 'completion': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"models/sentiment:50agree/smollm2:135m/inference_outputs/elated-grass-15\")\n",
    "\n",
    "print(\"Dataset structure\")\n",
    "print(dataset)\n",
    "\n",
    "print(\"First row\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset sentences_50agree with 4846 training samples.\n"
     ]
    }
   ],
   "source": [
    "from src.data.process_datasets import get_processed_hf_dataset\n",
    "\n",
    "sentences, true_labels = get_processed_hf_dataset(dataset=\"sentiment\", split_mode=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with PEFT LoRA stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
     ]
    }
   ],
   "source": [
    "from src.models.hf_utils import load_model_from_hf\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model, tokenizer = load_model_from_hf(\"llama3.2:1b\")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 45.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 25 Mar 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Which is bigger, the moon or the sun?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The sun.<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "\n",
    "chat1 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, the moon or the sun?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The sun.\"}\n",
    "]\n",
    "chat2 = [\n",
    "    {\"role\": \"user\", \"content\": \"Which is bigger, a virus or a bacterium?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"A bacterium.\"}\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_dict({\"chat\": [chat1, chat2]})\n",
    "dataset = dataset.map(lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)})\n",
    "print(dataset['formatted_chat'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
