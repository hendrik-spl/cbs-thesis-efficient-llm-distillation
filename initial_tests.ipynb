{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeCarbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n",
      "[codecarbon INFO @ 09:37:51] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 09:37:51] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 09:37:51] No CPU tracking mode found. Falling back on CPU constant mode. \n",
      " Mac OS and ARM processor detected: Please enable PowerMetrics sudo to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 09:37:51] CPU Model on constant consumption mode: Apple M1\n",
      "[codecarbon INFO @ 09:37:51] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 09:37:51] No GPU found.\n",
      "[codecarbon INFO @ 09:37:51] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 09:37:51]   Platform system: macOS-15.3-arm64-arm-64bit\n",
      "[codecarbon INFO @ 09:37:51]   Python version: 3.11.7\n",
      "[codecarbon INFO @ 09:37:51]   CodeCarbon version: 2.8.3\n",
      "[codecarbon INFO @ 09:37:51]   Available RAM : 8.000 GB\n",
      "[codecarbon INFO @ 09:37:51]   CPU count: 8\n",
      "[codecarbon INFO @ 09:37:51]   CPU model: Apple M1\n",
      "[codecarbon INFO @ 09:37:51]   GPU count: None\n",
      "[codecarbon INFO @ 09:37:51]   GPU model: None\n",
      "[codecarbon INFO @ 09:37:51] Saving emissions data to file /Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/metrics/emissions/emissions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.8638 - loss: 0.4724\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9543 - loss: 0.1494\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9669 - loss: 0.1116\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9732 - loss: 0.0859\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9768 - loss: 0.0743\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9794 - loss: 0.0651\n",
      "Epoch 7/10\n",
      "\u001b[1m 355/1875\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9829 - loss: 0.0542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:38:06] Energy consumed for RAM : 0.000000 kWh. RAM Power : 0.11040115356445312 W\n",
      "[codecarbon INFO @ 09:38:06] Energy consumed for all CPUs : 0.000021 kWh. Total CPU Power : 5.0 W\n",
      "[codecarbon INFO @ 09:38:06] 0.000021 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9814 - loss: 0.0574\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9838 - loss: 0.0491\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9851 - loss: 0.0459\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9864 - loss: 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon INFO @ 09:38:15] Energy consumed for RAM : 0.000001 kWh. RAM Power : 0.03935050964355469 W\n",
      "[codecarbon INFO @ 09:38:15] Energy consumed for all CPUs : 0.000033 kWh. Total CPU Power : 5.0 W\n",
      "[codecarbon INFO @ 09:38:15] 0.000033 kWh of electricity used since the beginning.\n",
      "/Users/hendriksippel/Documents/Repositories/cbs-thesis-efficient-llm-distillation/.venv/lib/python3.11/site-packages/codecarbon/output_methods/file.py:52: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, pd.DataFrame.from_records([dict(total.values)])])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "with EmissionsTracker(\n",
    "    project_name=\"model-distillation\", \n",
    "    experiment_id=\"123\", \n",
    "    tracking_mode=\"process\",\n",
    "    output_dir=\"metrics/emissions\"\n",
    "    ) as tracker:\n",
    "    model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "    model.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracked final energy consumption: 3.342663552604511e-05 kWh\n",
      "Tracked final CO2 emissions: 5.0691492775247405e-06 kgCO2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tracked final energy consumption: {tracker._total_energy.kWh} kWh\")\n",
    "print(f\"Tracked final CO2 emissions: {tracker.final_emissions} kgCO2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "MODEL = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The letter \"a\" appears twice in the word \"banana\".\n"
     ]
    }
   ],
   "source": [
    "# The following code also calls the appropriate Ollama endpoint and activates the model.\n",
    "\n",
    "url = \"http://localhost:11434/api/chat\"\n",
    "\n",
    "def ollama(model, prompt):\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"message\"][\"content\"]\n",
    "    else:\n",
    "        return \"Error\"\n",
    "\n",
    "response = ollama(\n",
    "    model=MODEL,\n",
    "    prompt=\"How often does the letter a appear in banana?\"\n",
    "    )\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      PROCESSOR    UNTIL              \n",
      "llama3.2:1b    baf6a787fdff    2.8 GB    100% GPU     4 minutes from now    \n"
     ]
    }
   ],
   "source": [
    "# To list all actively running models, you can use the following code:\n",
    "!ollama ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[?25l\u001b[?25h\u001b[2K\u001b[1G\u001b[?25hNAME    ID    SIZE    PROCESSOR    UNTIL \n"
     ]
    }
   ],
   "source": [
    "# To stop the Ollama server, execute the following command:\n",
    "!ollama stop llama3.2:1b\n",
    "\n",
    "# Confirmation\n",
    "!ollama ps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
